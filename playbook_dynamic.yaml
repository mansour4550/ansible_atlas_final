playbook_dynamic
---
- name: Install and configure prerequisites on all nodes
  hosts: all
  become: yes
  vars:
    atlas_version: "2.4.0"
    atlas_install_dir: "/opt/apache-atlas-{{ atlas_version }}"
    solr_version: "8.11.2"
    solr_install_dir: "/opt/solr-{{ solr_version }}"
    zookeeper_quorum: "{{ groups['zookeeper_nodes'] | map('extract', hostvars, 'ansible_host') | map('regex_replace', '^(.*)$', '\\1:2181') | join(',') }}"
    kafka_broker: "{{ hostvars['slave2']['ansible_host'] }}:9092"
    solr_url: "http://{{ hostvars['edge1']['ansible_host'] }}:8983/solr"
    atlas_user: "hadoopZetta"
    hbase_dir: "/opt/hbase-2.5.5-hadoop3"
    kafka_dir: "/opt/kafka_2.11-2.2.0"
  tasks:
    - name: Ensure hadoopZetta user exists
      user:
        name: "{{ atlas_user }}"
        state: present
        create_home: yes
        shell: /bin/bash

    - name: Install required packages as root
      apt:
        name: "{{ item }}"
        state: present
        update_cache: yes
      loop:
        - maven
        - wget
        - tar
        - curl
        - openjdk-8-jdk
        - python3

- name: Start HBase on master1
  hosts: master1
  become: yes
  vars:
    atlas_user: "hadoopZetta"
    hbase_dir: "/opt/hbase-2.5.5-hadoop3"
  tasks:
    - name: Check if HBase is already installed
      stat:
        path: "{{ hbase_dir }}/bin/start-hbase.sh"
      register: hbase_installed

    - name: Start HBase
      become_user: "{{ atlas_user }}"
      command: "{{ hbase_dir }}/bin/start-hbase.sh"
      when: hbase_installed.stat.exists
      register: hbase_start
      failed_when: hbase_start.rc != 0 and "'already running' not in hbase_start.stderr"

    - name: Wait for HBase to be ready
      become_user: "{{ atlas_user }}"
      command: "{{ hbase_dir }}/bin/hbase shell -c 'status'"
      register: hbase_status
      until: hbase_status.rc == 0
      retries: 5
      delay: 10
      when: hbase_installed.stat.exists

    - name: Grant HBase permissions to Atlas
      become_user: "{{ atlas_user }}"
      command: "echo \"grant 'atlas', 'RWXCA', 'atlas'\" | {{ hbase_dir }}/bin/hbase shell"
      when: hbase_installed.stat.exists
      register: hbase_grant
      failed_when: hbase_grant.rc != 0

- name: Configure and start Solr on slave2 and edge1
  hosts: slave2,edge1
  become: yes
  vars:
    atlas_user: "hadoopZetta"
    solr_version: "8.11.2"
    solr_install_dir: "/opt/solr-{{ solr_version }}"
    zookeeper_quorum: "{{ groups['zookeeper_nodes'] | map('extract', hostvars, 'ansible_host') | map('regex_replace', '^(.*)$', '\\1:2181') | join(',') }}"
  tasks:
    - name: Check if Solr is already installed
      stat:
        path: "{{ solr_install_dir }}/bin/solr"
      register: solr_installed

    - name: Start Solr in cloud mode
      become_user: "{{ atlas_user }}"
      command: "{{ solr_install_dir }}/bin/solr start -cloud -z {{ zookeeper_quorum }} -p 8983"
      when: solr_installed.stat.exists
      register: solr_start
      failed_when: solr_start.rc != 0 and "'already running' not in solr_start.stderr"

    - name: Wait for Solr to be ready
      become_user: "{{ atlas_user }}"
      wait_for:
        host: "{{ ansible_default_ipv4.address }}"
        port: 8983
        delay: 5
        timeout: 60
      when: solr_installed.stat.exists

    - name: Create Solr collections (on edge1 only)
      become_user: "{{ atlas_user }}"
      command: "{{ solr_install_dir }}/bin/solr create -c {{ item }} -n data-driven-schema-configs"
      when: inventory_hostname == "edge1" and solr_installed.stat.exists
      loop:
        - vertex_index
        - edge_index
        - fulltext_index
      register: solr_create
      failed_when: solr_create.rc != 0 and "'already exists' not in solr_create.stderr"

- name: Configure and start Kafka on slave2
  hosts: slave2
  become: yes
  vars:
    kafka_install_dir: "/opt/kafka_2.11-2.2.0"
    kafka_user: "hadoopZetta"
    kafka_group: "hadoopZetta"
    zookeeper_quorum: "{{ groups['zookeeper_nodes'] | map('extract', hostvars, 'ansible_host') | map('regex_replace', '^(.*)$', '\\1:2181') | join(',') }}"
  tasks:
    - name: Check if Kafka is already installed
      stat:
        path: "{{ kafka_install_dir }}/bin/kafka-server-start.sh"
      register: kafka_installed

    - name: Ensure Kafka config directory exists
      file:
        path: "{{ kafka_install_dir }}/config"
        state: directory
        mode: '0755'

    - name: Configure Kafka server.properties
      ansible.builtin.copy:
        content: |
          # Licensed to the Apache Software Foundation (ASF) under one or more
          # contributor license agreements.  See the NOTICE file distributed with
          # this work for additional information regarding copyright ownership.
          # The ASF licenses this file to You under the Apache License, Version 2.0
          # (the "License"); you may not use this file except in compliance with
          # the License.  You may obtain a copy of the License at
          #
          #    http://www.apache.org/licenses/LICENSE-2.0
          #
          # Unless required by applicable law or agreed to in writing, software
          # distributed under the License is distributed on an "AS IS" BASIS,
          # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
          # See the License for the specific language governing permissions and
          # limitations under the License.

          # see kafka.server.KafkaConfig for additional details and defaults

          ############################# Server Basics #############################

          # The id of the broker. This must be set to a unique integer for each broker.
          broker.id=0

          ############################# Socket Server Settings #############################

          # The address the socket server listens on. It will get the value returned from
          # java.net.InetAddress.getCanonicalHostName() if not configured.
          #   FORMAT:
          #     listeners = listener_name://host_name:port
          #   EXAMPLE:
          #     listeners = PLAINTEXT://your.host.name:9092
          listeners=PLAINTEXT://{{ ansible_default_ipv4.address }}:9092

          # Hostname and port the broker will advertise to producers and consumers. If not set,
          # it uses the value for "listeners" if configured.  Otherwise, it will use the value
          # returned from java.net.InetAddress.getCanonicalHostName().
          #advertised.listeners=PLAINTEXT://your.host.name:9092

          # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
          #listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

          # The number of threads that the server uses for receiving requests from the network and sending responses to the network
          num.network.threads=3

          # The number of threads that the server uses for processing requests, which may include disk I/O
          num.io.threads=8

          # The send buffer (SO_SNDBUF) used by the socket server
          socket.send.buffer.bytes=102400

          # The receive buffer (SO_RCVBUF) used by the socket server
          socket.receive.buffer.bytes=102400

          # The maximum size of a request that the socket server will accept (protection against OOM)
          socket.request.max.bytes=104857600

          ############################# Log Basics #############################

          # A comma separated list of directories under which to store log files
          log.dirs=/var/lib/kafka-logs

          # The default number of log partitions per topic. More partitions allow greater
          # parallelism for consumption, but this will also result in more files across
          # the brokers.
          num.partitions=1

          # The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
          # This value is recommended to be increased for installations with data dirs located in RAID array.
          num.recovery.threads.per.data.dir=1

          ############################# Internal Topic Settings  #############################
          # The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
          # For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.
          offsets.topic.replication.factor=1
          transaction.state.log.replication.factor=1
          transaction.state.log.min.isr=1

          ############################# Log Flush Policy #############################

          # Messages are immediately written to the filesystem but by default we only fsync() to sync
          # the OS cache lazily. The following configurations control the flush of data to disk.
          # There are a few important trade-offs here:
          #    1. Durability: Unflushed data may be lost if you are not using replication.
          #    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
          #    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
          # The settings below allow one to configure the flush policy to flush data after a period of time or
          # every N messages (or both). This can be done globally and overridden on a per-topic basis.

          # The number of messages to accept before forcing a flush of data to disk
          #log.flush.interval.messages=10000

          # The maximum amount of time a message can sit in a log before we force a flush
          #log.flush.interval.ms=1000

          ############################# Log Retention Policy #############################

          # The following configurations control the disposal of log segments. The policy can
          # be set to delete segments after a period of time, or after a given size has accumulated.
          # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
          # from the end of the log.

          # The minimum age of a log file to be eligible for deletion due to age
          log.retention.hours=168

          # A size-based retention policy for logs. Segments are pruned from the log unless the remaining
          # segments drop below log.retention.bytes. Functions independently of log.retention.hours.
          #log.retention.bytes=1073741824

          # The maximum size of a log segment file. When this size is reached a new log segment will be created.
          log.segment.bytes=1073741824

          # The interval at which log segments are checked to see if they can be deleted according
          # to the retention policies
          log.retention.check.interval.ms=300000

          ############################# Zookeeper #############################

          # Zookeeper connection string (see zookeeper docs for details).
          # This is a comma separated host:port pairs, each corresponding to a zk
          # server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
          # You can also append an optional chroot string to the urls to specify the
          # root directory for all kafka znodes.
          zookeeper.connect={{ zookeeper_quorum }}

          # Timeout in ms for connecting to zookeeper
          zookeeper.connection.timeout.ms=6000

          ############################# Group Coordinator Settings ####################
        dest: "{{ kafka_install_dir }}/config/server.properties"
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
        mode: '0644'

    - name: Ensure Kafka log directory exists
      file:
        path: "/var/lib/kafka-logs"
        state: directory
        mode: '0755'
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"

    - name: Change ownership of Kafka directory to hadoopZetta
      file:
        path: "{{ kafka_install_dir }}"
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
        recurse: yes

    - name: Start Kafka
      become_user: "{{ kafka_user }}"
      command: "{{ kafka_install_dir }}/bin/kafka-server-start.sh -daemon {{ kafka_install_dir }}/config/server.properties"
      when: kafka_installed.stat.exists
      register: kafka_start
      failed_when: kafka_start.rc != 0 and "'already running' not in kafka_start.stderr"

    - name: Wait for Kafka to be ready
      become_user: "{{ kafka_user }}"
      wait_for:
        host: "{{ ansible_default_ipv4.address }}"
        port: 9092
        delay: 5
        timeout: 60
      when: kafka_installed.stat.exists

    - name: Create Kafka topics for Atlas
      become_user: "{{ kafka_user }}"
      command: "{{ kafka_install_dir }}/bin/kafka-topics.sh --create --topic {{ item }} --bootstrap-server {{ ansible_default_ipv4.address }}:9092 --partitions 1 --replication-factor 1"
      when: kafka_installed.stat.exists
      loop:
        - ATLAS_HOOK
        - ATLAS_ENTITIES
      register: kafka_topics
      failed_when: kafka_topics.rc != 0 and "'already exists' not in kafka_topics.stderr"

- name: Install and configure Atlas on master nodes
  hosts: master1,master2
  become: yes
  vars:
    atlas_version: "2.4.0"
    atlas_install_dir: "/opt/apache-atlas-{{ atlas_version }}"
    atlas_source_dir: "/home/hadoopZetta/apache-atlas-sources-{{ atlas_version }}"
    zookeeper_quorum: "{{ groups['zookeeper_nodes'] | map('extract', hostvars, 'ansible_host') | map('regex_replace', '^(.*)$', '\\1:2181') | join(',') }}"
    kafka_broker: "{{ hostvars['slave2']['ansible_host'] }}:9092"
    solr_url: "http://{{ hostvars['edge1']['ansible_host'] }}:8983/solr"
    atlas_user: "hadoopZetta"
    hbase_dir: "/opt/hbase-2.5.5-hadoop3"
    java_home: "/usr/lib/jvm/java-8-openjdk-amd64"
    node_version: "16.20.2"
    node_gyp_cache: "/home/hadoopZetta/.cache/node-gyp"
  tasks:
    - name: Ensure hadoopZetta user exists
      ansible.builtin.user:
        name: "{{ atlas_user }}"
        state: present
        create_home: yes
        shell: /bin/bash

    - name: Install required packages as root
      ansible.builtin.apt:
        name:
          - maven
          - make
          - g++
          - python3
        state: present
        update_cache: yes

    - name: Download Node.js
      ansible.builtin.get_url:
        url: "https://nodejs.org/dist/v{{ node_version }}/node-v{{ node_version }}-linux-x64.tar.xz"
        dest: "/tmp/node-v{{ node_version }}-linux-x64.tar.xz"
        mode: '0644'

    - name: Extract Node.js
      ansible.builtin.unarchive:
        src: "/tmp/node-v{{ node_version }}-linux-x64.tar.xz"
        dest: "/usr/local"
        remote_src: yes
        owner: "{{ atlas_user }}"
        group: "{{ atlas_user }}"
        mode: '0755'

    - name: Ensure Node.js binaries are in PATH
      ansible.builtin.lineinfile:
        path: "/home/{{ atlas_user }}/.bashrc"
        line: 'export PATH=/usr/local/node-v{{ node_version }}-linux-x64/bin:$PATH'
        owner: "{{ atlas_user }}"
        group: "{{ atlas_user }}"
        mode: '0644'
        create: yes

    - name: Create Node.js cache directory
      ansible.builtin.file:
        path: "{{ node_gyp_cache }}"
        state: directory
        owner: "{{ atlas_user }}"
        group: "{{ atlas_user }}"
        mode: '0755'

    - name: Check if Atlas is already installed
      ansible.builtin.stat:
        path: "{{ atlas_install_dir }}/bin/atlas_start.py"
      register: atlas_installed

    - name: Create Atlas source directory
      ansible.builtin.file:
        path: "{{ atlas_source_dir }}"
        state: directory
        owner: "{{ atlas_user }}"
        group: "{{ atlas_user }}"
        mode: '0755'
      when: not atlas_installed.stat.exists

    - name: Download Apache Atlas source
      ansible.builtin.get_url:
        url: "https://dlcdn.apache.org/atlas/{{ atlas_version }}/apache-atlas-{{ atlas_version }}-sources.tar.gz"
        dest: "/home/hadoopZetta/apache-atlas-{{ atlas_version }}-sources.tar.gz"
        owner: "{{ atlas_user }}"
        group: "{{ atlas_user }}"
        mode: '0644'
      when: not atlas_installed.stat.exists

    - name: Extract Atlas source
      ansible.builtin.unarchive:
        src: "/home/hadoopZetta/apache-atlas-{{ atlas_version }}-sources.tar.gz"
        dest: "/home/hadoopZetta"
        remote_src: yes
        owner: "{{ atlas_user }}"
        group: "{{ atlas_user }}"
        mode: '0755'
      when: not atlas_installed.stat.exists

    - name: Ensure Atlas source directory is writable
      ansible.builtin.file:
        path: "{{ atlas_source_dir }}"
        owner: "{{ atlas_user }}"
        group: "{{ atlas_user }}"
        mode: 'u+rwx'
        recurse: yes
      when: not atlas_installed.stat.exists

    - name: Build Atlas with Maven
      ansible.builtin.command:
        cmd: mvn clean -DskipTests package -Pdist
        chdir: "{{ atlas_source_dir }}"
      environment:
        MAVEN_OPTS: "-Xms2g -Xmx2g"
        HOME: "/home/{{ atlas_user }}"
        NODE_GYP_CACHE: "{{ node_gyp_cache }}"
        PATH: "/usr/local/node-v{{ node_version }}-linux-x64/bin:{{ ansible_env.PATH }}"
      args:
        creates: "{{ atlas_source_dir }}/distro/target/apache-atlas-{{ atlas_version }}-server.tar.gz"
      become_user: "{{ atlas_user }}"
      when: not atlas_installed.stat.exists

    - name: Create Atlas installation directory
      ansible.builtin.file:
        path: "{{ atlas_install_dir }}"
        state: directory
        owner: "{{ atlas_user }}"
        group: "{{ atlas_user }}"
        mode: '0755'
      when: not atlas_installed.stat.exists

    - name: Install Atlas binaries
      ansible.builtin.unarchive:
        src: "{{ atlas_source_dir }}/distro/target/apache-atlas-{{ atlas_version }}-server.tar.gz"
        dest: "{{ atlas_install_dir }}"
        remote_src: yes
        owner: "{{ atlas_user }}"
        group: "{{ atlas_user }}"
        mode: '0755'
        extra_opts: [--strip-components=1]
        creates: "{{ atlas_install_dir }}/bin/atlas_start.py"
      when: not atlas_installed.stat.exists

    - name: Configure atlas-application.properties
      ansible.builtin.copy:
        content: |
          #
          # Licensed to the Apache Software Foundation (ASF) under one
          # or more contributor license agreements.  See the NOTICE file
          # distributed with this work for additional information
          # regarding copyright ownership.  The ASF licenses this file
          # to you under the Apache License, Version 2.0 (the
          # "License"); you may not use this file except in compliance
          # with the License.  You may obtain a copy of the License at
          #
          #     http://www.apache.org/licenses/LICENSE-2.0
          #
          # Unless required by applicable law or agreed to in writing, software
          # distributed under the License is distributed on an "AS IS" BASIS,
          # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
          # See the License for the specific language governing permissions and
          # limitations under the License.
          #

          #########  Graph Database Configs  #########

          # Graph Database
          atlas.graphdb.backend=org.apache.atlas.repository.graphdb.janus.AtlasJanusGraphDatabase

          # Graph Storage
          atlas.graph.storage.backend=hbase2
          atlas.graph.storage.hbase.table=apache_atlas_janus

          # Hbase
          atlas.graph.storage.hostname={{ zookeeper_quorum }}
          atlas.graph.storage.hbase.regions-per-server=1
          atlas.cluster.name=primary
          atlas.server.ha.enabled=true
          atlas.server.ids=id1,id2
          atlas.server.address.id1={{ hostvars['master1']['ansible_host'] }}:21000
          atlas.server.address.id2={{ hostvars['master2']['ansible_host'] }}:21000
          atlas.hbase.hook.enabled=true

          # Entity audit repository
          atlas.EntityAuditRepository.impl=org.apache.atlas.repository.audit.HBaseBasedAuditRepository

          # Graph Search Index
          atlas.graph.index.search.backend=solr

          # Solr
          atlas.graph.index.search.solr.mode=cloud
          atlas.graph.index.search.solr.zookeeper-url={{ zookeeper_quorum }}
          atlas.graph.index.search.solr.zookeeper-connect-timeout=60000
          atlas.graph.index.search.solr.zookeeper-session-timeout=60000
          atlas.graph.index.search.solr.wait-searcher=false
          atlas.graph.index.search.solr.http-urls={{ solr_url }}

          # Solr-specific configuration property
          atlas.graph.index.search.max-result-set-size=150

          #########  Notification Configs  #########
          atlas.kafka.data=${sys:atlas.home}/data/kafka
          atlas.kafka.zookeeper.connect={{ zookeeper_quorum }}
          atlas.kafka.bootstrap.servers={{ kafka_broker }}
          atlas.kafka.zookeeper.session.timeout.ms=400
          atlas.kafka.zookeeper.connection.timeout.ms=200
          atlas.kafka.zookeeper.sync.time.ms=20
          atlas.kafka.auto.commit.interval.ms=1000
          atlas.kafka.hook.group.id=atlas

          atlas.kafka.enable.auto.commit=false
          atlas.kafka.auto.offset.reset=earliest
          atlas.kafka.session.timeout.ms=30000
          atlas.kafka.offsets.topic.replication.factor=1
          atlas.kafka.poll.timeout.ms=1000

          atlas.notification.create.topics=true
          atlas.notification.replicas=1
          atlas.notification.topics=ATLAS_HOOK,ATLAS_ENTITIES
          atlas.notification.log.failed.messages=true
          atlas.notification.consumer.retry.interval=500
          atlas.notification.hook.retry.interval=1000

          #########  Security Properties  #########
          atlas.enableTLS=false

          # Authentication config
          atlas.authentication.method.kerberos=false
          atlas.authentication.method.file=true
          atlas.authentication.method.ldap.type=none
          atlas.authentication.method.file.filename=${sys:atlas.home}/conf/users-credentials.properties

          #########  Server Properties  #########
          atlas.rest.address=http://{{ ansible_default_ipv4.address }}:21000

          #########  Entity Audit Configs  #########
          atlas.audit.hbase.tablename=apache_atlas_entity_audit
          atlas.audit.zookeeper.session.timeout.ms=1000
          atlas.audit.hbase.zookeeper.quorum={{ groups['zookeeper_nodes'] | map('extract', hostvars, 'ansible_host') | join(',') }}

          ######### Atlas Authorization #########
          atlas.authorizer.impl=simple
          atlas.authorizer.simple.authz.policy.file=atlas-simple-authz-policy.json

          #########  CSRF Configs  #########
          atlas.rest-csrf.enabled=true
          atlas.rest-csrf.browser-useragents-regex=^Mozilla.*,^Opera.*,^Chrome.*
          atlas.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD,TRACE
          atlas.rest-csrf.custom-header=X-XSRF-HEADER

          #########  UI Configuration ########
          atlas.ui.default.version=v1

          atlas.hook.hive.synchronous=false
          atlas.hook.hive.numRetries=3
          atlas.hook.hive.queueSize=10000
        dest: "{{ atlas_install_dir }}/conf/atlas-application.properties"
        owner: "{{ atlas_user }}"
        group: "{{ atlas_user }}"
        mode: '0644'

    - name: Configure atlas-env.sh
      ansible.builtin.blockinfile:
        path: "{{ atlas_install_dir }}/conf/atlas-env.sh"
        block: |
          #!/bin/bash
          export JAVA_HOME={{ java_home }}
          export ATLAS_HOME_DIR={{ atlas_install_dir }}
          export ATLAS_LOG_DIR={{ atlas_install_dir }}/logs
          export ATLAS_PID_DIR={{ atlas_install_dir }}/logs
          export ATLAS_DATA_DIR={{ atlas_install_dir }}/data
          export ATLAS_OPTS="-Djanusgraph.index.search.solr.mode=http -Djanusgraph.index.search.solr.http-urls={{ solr_url }}"
          export ATLAS_SERVER_OPTS="-server -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+CMSClassUnloadingEnabled -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+PrintTenuringDistribution -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath={{ atlas_install_dir }}/logs/dumps/atlas_server.hprof -Xloggc:{{ atlas_install_dir }}/logs/gc-worker.log -verbose:gc -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=1m -XX:+PrintGCDetails -XX:+PrintHeapAtGC -XX:+PrintGCTimeStamps"
          export ATLAS_SERVER_HEAP="-Xms4g -Xmx4g -XX:MaxNewSize=1g -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=512m"
          export MANAGE_LOCAL_HBASE=false
          export HBASE_CONF_DIR={{ atlas_install_dir }}/conf/hbase
          export MANAGE_LOCAL_SOLR=false
          export MANAGE_EMBEDDED_CASSANDRA=false
          export MANAGE_LOCAL_ELASTICSEARCH=false
        marker: "# {mark} ANSIBLE MANAGED BLOCK"
        owner: "{{ atlas_user }}"
        group: "{{ atlas_user }}"
        mode: '0755'

    - name: Ensure Atlas hbase conf directory exists
      ansible.builtin.file:
        path: "{{ atlas_install_dir }}/conf/hbase"
        state: directory
        owner: "{{ atlas_user }}"
        group: "{{ atlas_user }}"
        mode: '0755'

    - name: Create hbase-site.xml in Atlas hbase conf directory
      ansible.builtin.copy:
        content: |
          <?xml version="1.0"?>
          <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
          <configuration>
            <property>
              <name>hbase.rootdir</name>
              <value>hdfs://mycluster/hbase</value>
            </property>
            <property>
              <name>hbase.cluster.distributed</name>
              <value>true</value>
            </property>
            <property>
              <name>hbase.wal.provider</name>
              <value>filesystem</value>
            </property>
            <property>
              <name>hbase.zookeeper.quorum</name>
              <value>{{ groups['zookeeper_nodes'] | map('extract', hostvars, 'ansible_host') | join(',') }}</value>
            </property>
            <property>
              <name>dfs.replication</name>
              <value>2</value>
            </property>
            <property>
              <name>hbase.zookeeper.property.clientPort</name>
              <value>2181</value>
            </property>
            <property>
              <name>hbase.zookeeper.property.dataDir</name>
              <value>/var/lib/zookeeper</value>
            </property>
          </configuration>
        dest: "{{ atlas_install_dir }}/conf/hbase/hbase-site.xml"
        owner: "{{ atlas_user }}"
        group: "{{ atlas_user }}"
        mode: '0644'

    - name: Check if hbase-site.xml exists in HBase conf directory
      ansible.builtin.stat:
        path: "{{ hbase_dir }}/conf/hbase-site.xml"
      register: hbase_site_exists

    - name: Create hbase-site.xml in HBase conf directory if it does not exist
      ansible.builtin.copy:
        content: |
          <?xml version="1.0"?>
          <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
          <configuration>
            <property>
              <name>hbase.rootdir</name>
              <value>hdfs://mycluster/hbase</value>
            </property>
            <property>
              <name>hbase.cluster.distributed</name>
              <value>true</value>
            </property>
            <property>
              <name>hbase.wal.provider</name>
              <value>filesystem</value>
            </property>
            <property>
              <name>hbase.zookeeper.quorum</name>
              <value>{{ groups['zookeeper_nodes'] | map('extract', hostvars, 'ansible_host') | join(',') }}</value>
            </property>
            <property>
              <name>dfs.replication</name>
              <value>2</value>
            </property>
            <property>
              <name>hbase.zookeeper.property.clientPort</name>
              <value>2181</value>
            </property>
            <property>
              <name>hbase.zookeeper.property.dataDir</name>
              <value>/var/lib/zookeeper</value>
            </property>
          </configuration>
        dest: "{{ hbase_dir }}/conf/hbase-site.xml"
        owner: "{{ atlas_user }}"
        group: "{{ atlas_user }}"
        mode: '0644'
      when: not hbase_site_exists.stat.exists

    - name: Create users-credentials.properties
      ansible.builtin.copy:
        content: |
          admin=ADMIN::password
        dest: "{{ atlas_install_dir }}/conf/users-credentials.properties"
        owner: "{{ atlas_user }}"
        group: "{{ atlas_user }}"
        mode: '0644'

    - name: Create atlas-simple-authz-policy.json
      ansible.builtin.copy:
        content: |
          {
            "roles": {
              "admin": {
                "users": ["admin"],
                "groups": [],
                "permissions": [
                  {"operation": "*"}
                ]
              }
            }
          }
        dest: "{{ atlas_install_dir }}/conf/atlas-simple-authz-policy.json"
        owner: "{{ atlas_user }}"
        group: "{{ atlas_user }}"
        mode: '0644'

    - name: Change ownership of Atlas directory to hadoopZetta
      ansible.builtin.file:
        path: "{{ atlas_install_dir }}"
        owner: "{{ atlas_user }}"
        group: "{{ atlas_user }}"
        recurse: yes
        mode: '0755'

    - name: Start Atlas
      ansible.builtin.command:
        cmd: python3 {{ atlas_install_dir }}/bin/atlas_start.py
        chdir: "{{ atlas_install_dir }}"
      environment:
        DISPLAY: ""
      become_user: "{{ atlas_user }}"
      register: atlas_start
      failed_when: atlas_start.rc != 0 and "'already running' not in atlas_start.stderr"
      timeout: 120

    - name: Check if Atlas is running
      ansible.builtin.shell:
        cmd: ps aux | grep -v grep | grep "org.apache.atlas.Atlas" || exit 1
      become_user: "{{ atlas_user }}"
      register: atlas_process
      failed_when: atlas_process.rc != 0
      changed_when: false

    - name: Display Atlas UI verification message
      ansible.builtin.debug:
        msg: "Verify in Atlas UI at http://{{ hostvars['master1']['ansible_host'] }}:21000 or http://{{ hostvars['master2']['ansible_host'] }}:21000"
      when: inventory_hostname == 'master1'